{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERTweetKFold.ipynb\n",
    "\n",
    "### Designation: KFold Cross-Validation Script\n",
    "\n",
    "    Purpose: 10-fold cross validate BERTweet with our current hyperparameter settings, and record the data.\n",
    "\n",
    "- Requirements:\n",
    "    \n",
    "    Packages: tensorflow, pandas, matplotlib, transformers, sklearn, os\n",
    "\n",
    "    Datasets (csv's): Tweets.csv\n",
    "\n",
    "    Saved Model Weight: bertweet9010.h5\n",
    "\n",
    "- This program will require an internet connection, as it will download the model and tokenizer from the HuggingFace model repository.\n",
    "\n",
    "- csv output: 'foldOutput.csv'\n",
    "    - Please note, all files referenced (input and output) will all be on the folder-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on KFolding\n",
    "\n",
    "- For detailed explanation, please refer to bertweet.ipynb\n",
    "\n",
    "- A GPU is strongly, strongly recommended for this program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TensorFlow Standalone Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-02 19:42:57.485201: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:57.490662: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:57.490927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    }
   ],
   "source": [
    "useCPU = False #Choose whether to use CPU or GPU for running the program\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "if useCPU:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing, downloading, and Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-06-02 19:42:58.343997: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-02 19:42:58.345006: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:58.345318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:58.345595: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:58.755169: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:58.755599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:58.755612: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-06-02 19:42:58.755982: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:0a:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-06-02 19:42:58.756023: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9579 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:0a:00.0, compute capability: 8.6\n",
      "2022-06-02 19:43:00.404676: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 134309376 \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592899    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 134,902,275\n",
      "Trainable params: 134,902,275\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import InputExample, InputFeatures\n",
    "with tf.device('/GPU:0'):\n",
    "    model = TFAutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\",num_labels=3,problem_type=\"multi_label_classification\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",num_labels=3)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Read in dataset: Tweets.csv (our dataset for training purposes), and clean up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text sentiment\n",
       "0                    I`d have responded, if I were going   neutral\n",
       "1          Sooo SAD I will miss you here in San Diego!!!  negative\n",
       "2                              my boss is bullying me...  negative\n",
       "3                         what interview! leave me alone  negative\n",
       "4       Sons of ****, why couldn`t they put them on t...  negative\n",
       "...                                                  ...       ...\n",
       "27476   wish we could come see u on Denver  husband l...  negative\n",
       "27477   I`ve wondered about rake to.  The client has ...  negative\n",
       "27478   Yay good for both of you. Enjoy the break - y...  positive\n",
       "27479                         But it was worth it  ****.  positive\n",
       "27480     All this flirting going on - The ATG smiles...   neutral\n",
       "\n",
       "[27481 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('../Dataset/Tweets.csv', encoding='ISO-8859-1')\n",
    "dataset_drop = dataset.drop(['textID', 'selected_text'], axis=1)\n",
    "dataset_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Extract and encode the dataset's label column into number-category encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "27476    0\n",
       "27477    0\n",
       "27478    2\n",
       "27479    2\n",
       "27480    1\n",
       "Name: sentiment, Length: 27481, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetSentimentEncode = dataset_drop['sentiment'].apply(lambda c: 0 if c == 'negative' else (1 if c=='neutral' else 2))\n",
    "datasetSentimentEncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compiling Training/Test split dataframes\n",
    "\n",
    "- 90:10 seeded split, note: same seeded split as bertweet9010.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                             DATA_COLUMN  LABEL_COLUMN\n",
       " 8775                                   blastinggg music.             1\n",
       " 8885    If it`s any consolation, you`re definitely on...             2\n",
       " 22325                    fun day with boo. short but fun             2\n",
       " 13024   Blow me away it IS raining harder here. Yay y...             1\n",
       " 17426   Lame remarks like 'I wonder if they like blon...             1\n",
       " ...                                                  ...           ...\n",
       " 16432                                   FC is back dear.             1\n",
       " 8964    tea...  Mmmm crispy but no cake  Have headpho...             1\n",
       " 5944                       thankyou very much, you rock!             2\n",
       " 5327                                i looking at failure             1\n",
       " 15305   happy mommas day . ging is so lucky to have a...             2\n",
       " \n",
       " [24732 rows x 2 columns],\n",
       "                                              DATA_COLUMN  LABEL_COLUMN\n",
       " 26493  I started X-Slimmer at eight this morning, it`...             0\n",
       " 11488                           why won`t the kids sleep             0\n",
       " 7782                     Jennnnnn richhhh wast to the ed             1\n",
       " 18241   LOL! I hate when that happens!! All hyped up ...             1\n",
       " 25540  Stuck on NJ Transit for the past twenty minute...             1\n",
       " ...                                                  ...           ...\n",
       " 11645                Yes. Nag twitter. HAHA  Thanks. LM.             2\n",
       " 23533   hm, this not a good medium for much more then...             0\n",
       " 6578     hapee mother`s day t all the mothers out there!             2\n",
       " 5338                        Aww maybe i traumatized her.             0\n",
       " 21793  _caruso So I took the polish off of the nail o...             0\n",
       " \n",
       " [2749 rows x 2 columns])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "xtrain,xtest,ytrain,ytest = train_test_split(dataset_drop['text'].astype(str), datasetSentimentEncode, test_size=0.1, random_state=21)\n",
    "trainDF = pd.DataFrame()\n",
    "testDF = pd.DataFrame()\n",
    "trainDF['DATA_COLUMN'] = xtrain\n",
    "trainDF['LABEL_COLUMN'] = ytrain\n",
    "testDF['DATA_COLUMN'] = xtest\n",
    "testDF['LABEL_COLUMN'] = ytest\n",
    "trainDF,testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Converting dataframes into supported input format for the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data_to_examples(train, test, DATA_COLUMN, LABEL_COLUMN): \n",
    "  train_InputExamples = train.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "  validation_InputExamples = test.apply(lambda x: InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this case\n",
    "                                                          text_a = x[DATA_COLUMN], \n",
    "                                                          text_b = None,\n",
    "                                                          label = x[LABEL_COLUMN]), axis = 1)\n",
    "  \n",
    "  return train_InputExamples, validation_InputExamples\n",
    "\n",
    "  \n",
    "def convert_examples_to_tf_dataset(examples, tokenizer, max_length=128):\n",
    "    features = [] # -> will hold InputFeatures to be converted later\n",
    "\n",
    "    for e in examples:\n",
    "        # Documentation is really strong for this method, so please take a look at it\n",
    "        input_dict = tokenizer.encode_plus(\n",
    "            e.text_a,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length, # truncates if len(s) > max_length\n",
    "            return_token_type_ids=True,\n",
    "            return_attention_mask=True,\n",
    "            pad_to_max_length=True, # pads to the right by default # CHECK THIS for pad_to_max_length\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        input_ids, token_type_ids, attention_mask = (input_dict[\"input_ids\"],\n",
    "            input_dict[\"token_type_ids\"], input_dict['attention_mask'])\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, label=e.label\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def gen():\n",
    "        for f in features:\n",
    "            yield (\n",
    "                {\n",
    "                    \"input_ids\": f.input_ids,\n",
    "                    \"attention_mask\": f.attention_mask,\n",
    "                    \"token_type_ids\": f.token_type_ids,\n",
    "                },\n",
    "                f.label,\n",
    "            )\n",
    "\n",
    "    return tf.data.Dataset.from_generator(\n",
    "        gen,\n",
    "        ({\"input_ids\": tf.int32, \"attention_mask\": tf.int32, \"token_type_ids\": tf.int32}, tf.int64),\n",
    "        (\n",
    "            {\n",
    "                \"input_ids\": tf.TensorShape([None]),\n",
    "                \"attention_mask\": tf.TensorShape([None]),\n",
    "                \"token_type_ids\": tf.TensorShape([None]),\n",
    "            },\n",
    "            tf.TensorShape([]),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. The call of the functions, and batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "DATA_COLUMN = 'DATA_COLUMN'\n",
    "LABEL_COLUMN = 'LABEL_COLUMN'\n",
    "train_InputExamples, validation_InputExamples = convert_data_to_examples(trainDF, testDF, DATA_COLUMN, LABEL_COLUMN)\n",
    "with tf.device('/GPU:0'):\n",
    "    train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "    #train_eval_data = train_data.batch(1)\n",
    "    train_data = train_data.shuffle(100).batch(32)#.repeat(2)\n",
    "    \n",
    "\n",
    "    validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "    validation_data = validation_data.batch(32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the model on the training set\n",
    "\n",
    "- train the model on the training set once to ensure that the training set is generalized for the kfolding results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773/773 [==============================] - 432s 544ms/step - loss: 0.5681 - accuracy: 0.7623 - val_loss: 0.4544 - val_accuracy: 0.8145\n"
     ]
    }
   ],
   "source": [
    "model.layers[0].trainable = True\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), #default: 3e-5\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "\n",
    "    model.fit(train_data, epochs=1, validation_data=validation_data)#callbacks=callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. (Optional) Save Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('bertweet9010.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Test Code) Generate predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"scores = {\\n                'test_accuracy': [],\\n                'test_precision_neg': [],\\n                'test_precision_neut': [],\\n                'test_precision_pos': [],\\n                'test_recall_neg': [],\\n                'test_recall_neut': [],\\n                'test_recall_pos': [],\\n                'test_f1_score_neg': [],\\n                'test_f1_score_neut': [],\\n                'test_f1_score_pos': []\\n          \\n          }\\nscores['test_accuracy'].append(model.metrics[1].result().numpy())\\npredictionsRaw = model.predict(train_eval_data)\\npredictions = pd.DataFrame(predictionsRaw['logits']).idxmax(axis=1)\\npredictions,ytrain\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''scores = {\n",
    "                'test_accuracy': [],\n",
    "                'test_precision_neg': [],\n",
    "                'test_precision_neut': [],\n",
    "                'test_precision_pos': [],\n",
    "                'test_recall_neg': [],\n",
    "                'test_recall_neut': [],\n",
    "                'test_recall_pos': [],\n",
    "                'test_f1_score_neg': [],\n",
    "                'test_f1_score_neut': [],\n",
    "                'test_f1_score_pos': []\n",
    "          \n",
    "          }\n",
    "scores['test_accuracy'].append(model.metrics[1].result().numpy())\n",
    "predictionsRaw = model.predict(train_eval_data)\n",
    "predictions = pd.DataFrame(predictionsRaw['logits']).idxmax(axis=1)\n",
    "predictions,ytrain'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from sklearn.metrics import precision_recall_fscore_support\\nprecision, recall, f1, _ = precision_recall_fscore_support(ytrain, predictions)\\nscores['test_precision_neg'].append(precision[0])\\nscores['test_precision_neut'].append(precision[1])\\nscores['test_precision_pos'].append(precision[2])\\nscores['test_recall_neg'].append(recall[0])\\nscores['test_recall_neut'].append(recall[1])\\nscores['test_recall_pos'].append(recall[2])\\nscores['test_f1_score_neg'].append(f1[0])\\nscores['test_f1_score_neut'].append(f1[1])\\nscores['test_f1_score_pos'].append(f1[2])\\nprint(scores)\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from sklearn.metrics import precision_recall_fscore_support\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(ytrain, predictions)\n",
    "scores['test_precision_neg'].append(precision[0])\n",
    "scores['test_precision_neut'].append(precision[1])\n",
    "scores['test_precision_pos'].append(precision[2])\n",
    "scores['test_recall_neg'].append(recall[0])\n",
    "scores['test_recall_neut'].append(recall[1])\n",
    "scores['test_recall_pos'].append(recall[2])\n",
    "scores['test_f1_score_neg'].append(f1[0])\n",
    "scores['test_f1_score_neut'].append(f1[1])\n",
    "scores['test_f1_score_pos'].append(f1[2])\n",
    "print(scores)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 10-Fold Cross Validation\n",
    "\n",
    "- we are folding on the 'training set', which is the 90% of the 90-10 train-test split.\n",
    "\n",
    "- predictions of the 1-fold ('test-set') are generated after each model is 'trained' on the 9-folds ('train-set') in the loop.\n",
    "\n",
    "- The predictions are evaluated against 'ground-truth', the fold's actual labels, and statistics recorded.\n",
    "\n",
    "Statistis are recorded and saved afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 390s 545ms/step - loss: 0.5821 - accuracy: 0.7540 - val_loss: 0.4869 - val_accuracy: 0.7967\n",
      "{'test_accuracy': [0.7966855], 'test_precision_neg': [0.8096551724137931], 'test_precision_neut': [0.7698237885462555], 'test_precision_pos': [0.8145065398335315], 'test_recall_neg': [0.8063186813186813], 'test_recall_neut': [0.7221074380165289], 'test_recall_pos': [0.8804627249357326], 'test_f1_score_neg': [0.8079834824501033], 'test_f1_score_neut': [0.7452025586353945], 'test_f1_score_pos': [0.8462013588634959]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 393s 549ms/step - loss: 0.5809 - accuracy: 0.7546 - val_loss: 0.4993 - val_accuracy: 0.8068\n",
      "{'test_accuracy': [0.7966855, 0.80679065], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 394s 552ms/step - loss: 0.5830 - accuracy: 0.7571 - val_loss: 0.4920 - val_accuracy: 0.8071\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 395s 552ms/step - loss: 0.5848 - accuracy: 0.7535 - val_loss: 0.5286 - val_accuracy: 0.7825\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 394s 550ms/step - loss: 0.5967 - accuracy: 0.7502 - val_loss: 0.5010 - val_accuracy: 0.7922\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044, 0.79215527], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538, 0.7561881188118812], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613, 0.7955346650998825], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017, 0.8243243243243243], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675, 0.8629943502824858], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778, 0.6838383838383838], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481, 0.8658064516129033], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653, 0.8060686015831134], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321, 0.7354698533405758], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403, 0.8445563247325362]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 386s 541ms/step - loss: 0.5773 - accuracy: 0.7598 - val_loss: 0.5038 - val_accuracy: 0.7857\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044, 0.79215527, 0.7856854], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538, 0.7561881188118812, 0.7931034482758621], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613, 0.7955346650998825, 0.8009205983889528], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017, 0.8243243243243243, 0.764505119453925], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675, 0.8629943502824858, 0.8202567760342369], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778, 0.6838383838383838, 0.6843657817109144], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481, 0.8658064516129033, 0.8900662251655629], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653, 0.8060686015831134, 0.8064516129032259], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321, 0.7354698533405758, 0.7380699893955461], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403, 0.8445563247325362, 0.8225214198286415]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 384s 537ms/step - loss: 0.5767 - accuracy: 0.7588 - val_loss: 0.4599 - val_accuracy: 0.8180\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044, 0.79215527, 0.7856854, 0.81803477], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538, 0.7561881188118812, 0.7931034482758621, 0.7925824175824175], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613, 0.7955346650998825, 0.8009205983889528, 0.8147762747138397], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017, 0.8243243243243243, 0.764505119453925, 0.8456632653061225], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675, 0.8629943502824858, 0.8202567760342369, 0.8548148148148148], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778, 0.6838383838383838, 0.6843657817109144, 0.7631578947368421], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481, 0.8658064516129033, 0.8900662251655629, 0.8588082901554405], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653, 0.8060686015831134, 0.8064516129032259, 0.8225231646471847], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321, 0.7354698533405758, 0.7380699893955461, 0.7881227981882234], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403, 0.8445563247325362, 0.8225214198286415, 0.8521850899742932]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 391s 545ms/step - loss: 0.5737 - accuracy: 0.7617 - val_loss: 0.5127 - val_accuracy: 0.7788\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044, 0.79215527, 0.7856854, 0.81803477, 0.77881116], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538, 0.7561881188118812, 0.7931034482758621, 0.7925824175824175, 0.7471116816431322], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613, 0.7955346650998825, 0.8009205983889528, 0.8147762747138397, 0.7628755364806867], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017, 0.8243243243243243, 0.764505119453925, 0.8456632653061225, 0.8307086614173228], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675, 0.8629943502824858, 0.8202567760342369, 0.8548148148148148, 0.8422575976845152], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778, 0.6838383838383838, 0.6843657817109144, 0.7631578947368421, 0.7088733798604188], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481, 0.8658064516129033, 0.8900662251655629, 0.8588082901554405, 0.8125802310654685], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653, 0.8060686015831134, 0.8064516129032259, 0.8225231646471847, 0.7918367346938776], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321, 0.7354698533405758, 0.7380699893955461, 0.7881227981882234, 0.7348837209302326], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403, 0.8445563247325362, 0.8225214198286415, 0.8521850899742932, 0.8215444516547696]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 391s 545ms/step - loss: 0.5742 - accuracy: 0.7593 - val_loss: 0.4944 - val_accuracy: 0.8027\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044, 0.79215527, 0.7856854, 0.81803477, 0.77881116, 0.8026688], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538, 0.7561881188118812, 0.7931034482758621, 0.7925824175824175, 0.7471116816431322, 0.7629911280101395], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613, 0.7955346650998825, 0.8009205983889528, 0.8147762747138397, 0.7628755364806867, 0.8370986920332937], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017, 0.8243243243243243, 0.764505119453925, 0.8456632653061225, 0.8307086614173228, 0.8054567022538552], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675, 0.8629943502824858, 0.8202567760342369, 0.8548148148148148, 0.8422575976845152, 0.8551136363636364], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778, 0.6838383838383838, 0.6843657817109144, 0.7631578947368421, 0.7088733798604188, 0.6991062562065541], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481, 0.8658064516129033, 0.8900662251655629, 0.8588082901554405, 0.8125802310654685, 0.8910761154855643], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653, 0.8060686015831134, 0.8064516129032259, 0.8225231646471847, 0.7918367346938776, 0.8064300066979238], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321, 0.7354698533405758, 0.7380699893955461, 0.7881227981882234, 0.7348837209302326, 0.761904761904762], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403, 0.8445563247325362, 0.8225214198286415, 0.8521850899742932, 0.8215444516547696, 0.8461059190031152]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "696/696 [==============================] - 391s 546ms/step - loss: 0.5791 - accuracy: 0.7598 - val_loss: 0.4874 - val_accuracy: 0.8019\n",
      "{'test_accuracy': [0.7966855, 0.80679065, 0.80711687, 0.78245044, 0.79215527, 0.7856854, 0.81803477, 0.77881116, 0.8026688, 0.8018601], 'test_precision_neg': [0.8096551724137931, 0.8102941176470588, 0.7798165137614679, 0.7515375153751538, 0.7561881188118812, 0.7931034482758621, 0.7925824175824175, 0.7471116816431322, 0.7629911280101395, 0.7925373134328358], 'test_precision_neut': [0.7698237885462555, 0.8078141499472017, 0.803921568627451, 0.8103225806451613, 0.7955346650998825, 0.8009205983889528, 0.8147762747138397, 0.7628755364806867, 0.8370986920332937, 0.7539906103286385], 'test_precision_pos': [0.8145065398335315, 0.8028335301062574, 0.8358585858585859, 0.7864406779661017, 0.8243243243243243, 0.764505119453925, 0.8456632653061225, 0.8307086614173228, 0.8054567022538552, 0.8794037940379403], 'test_recall_neg': [0.8063186813186813, 0.8043795620437956, 0.8298465829846583, 0.8557422969187675, 0.8629943502824858, 0.8202567760342369, 0.8548148148148148, 0.8422575976845152, 0.8551136363636364, 0.7913561847988078], 'test_recall_neut': [0.7221074380165289, 0.7463414634146341, 0.7387387387387387, 0.6480908152734778, 0.6838383838383838, 0.6843657817109144, 0.7631578947368421, 0.7088733798604188, 0.6991062562065541, 0.7966269841269841], 'test_recall_pos': [0.8804627249357326, 0.8900523560209425, 0.8745046235138706, 0.8810126582278481, 0.8658064516129033, 0.8900662251655629, 0.8588082901554405, 0.8125802310654685, 0.8910761154855643, 0.8173803526448362], 'test_f1_score_neg': [0.8079834824501033, 0.8073260073260072, 0.8040540540540542, 0.8002619515389653, 0.8060686015831134, 0.8064516129032259, 0.8225231646471847, 0.7918367346938776, 0.8064300066979238, 0.7919463087248323], 'test_f1_score_neut': [0.7452025586353945, 0.7758620689655173, 0.7699530516431926, 0.7201834862385321, 0.7354698533405758, 0.7380699893955461, 0.7881227981882234, 0.7348837209302326, 0.761904761904762, 0.7747226242161119], 'test_f1_score_pos': [0.8462013588634959, 0.8441961514587213, 0.8547449967721111, 0.831044776119403, 0.8445563247325362, 0.8225214198286415, 0.8521850899742932, 0.8215444516547696, 0.8461059190031152, 0.8472584856396865]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "split = 10\n",
    "\n",
    "#generate our folding sets\n",
    "X = xtrain.to_numpy()\n",
    "Y = ytrain.to_numpy()\n",
    "\n",
    "#score dict\n",
    "scores = {\n",
    "                'test_accuracy': [],\n",
    "                'test_precision_neg': [],\n",
    "                'test_precision_neut': [],\n",
    "                'test_precision_pos': [],\n",
    "                'test_recall_neg': [],\n",
    "                'test_recall_neut': [],\n",
    "                'test_recall_pos': [],\n",
    "                'test_f1_score_neg': [],\n",
    "                'test_f1_score_neut': [],\n",
    "                'test_f1_score_pos': []\n",
    "          \n",
    "          } \n",
    "\n",
    "#10-Fold cross validation loop\n",
    "for train_index,test_index in KFold(10).split(X):\n",
    "    #split the dataset into the folds\n",
    "    x_train,x_test = X[train_index],X[test_index]\n",
    "    y_train,y_test = Y[train_index],Y[test_index]\n",
    "\n",
    "    #Compiling train/test split dataframes\n",
    "    trainDF = pd.DataFrame()\n",
    "    testDF = pd.DataFrame()\n",
    "    trainDF['DATA_COLUMN'] = x_train\n",
    "    trainDF['LABEL_COLUMN'] = y_train\n",
    "    testDF['DATA_COLUMN'] = x_test\n",
    "    testDF['LABEL_COLUMN'] = y_test\n",
    "\n",
    "    #convert them into acceptable input formats (TensorFlow DataSet)\n",
    "    train_InputExamples, validation_InputExamples = convert_data_to_examples(trainDF, testDF, DATA_COLUMN, LABEL_COLUMN)\n",
    "    with tf.device('/GPU:0'):\n",
    "        train_data = convert_examples_to_tf_dataset(list(train_InputExamples), tokenizer)\n",
    "        #train_eval_data = train_data\n",
    "        train_data = train_data.shuffle(100).batch(32)#.repeat(2)\n",
    "\n",
    "        validation_data = convert_examples_to_tf_dataset(list(validation_InputExamples), tokenizer)\n",
    "        val_eval_data = validation_data.batch(1)\n",
    "        validation_data = validation_data.batch(32)\n",
    "\n",
    "    #Train the model\n",
    "    with tf.device('/GPU:0'):\n",
    "        model = TFAutoModelForSequenceClassification.from_pretrained(\"vinai/bertweet-base\",num_labels=3,problem_type=\"multi_label_classification\")\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0), #default: 3e-5\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy('accuracy')])\n",
    "        model.fit(train_data, epochs=1, validation_data=validation_data)\n",
    "        #record accuracy\n",
    "        scores['test_accuracy'].append(model.metrics[1].result().numpy())\n",
    "\n",
    "        #Predict the test set\n",
    "        predictionsRaw = model.predict(val_eval_data)\n",
    "        predictions = pd.DataFrame(predictionsRaw['logits']).idxmax(axis=1)\n",
    "\n",
    "    #calculate the statistics and record them\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions)\n",
    "    scores['test_precision_neg'].append(precision[0])\n",
    "    scores['test_precision_neut'].append(precision[1])\n",
    "    scores['test_precision_pos'].append(precision[2])\n",
    "    scores['test_recall_neg'].append(recall[0])\n",
    "    scores['test_recall_neut'].append(recall[1])\n",
    "    scores['test_recall_pos'].append(recall[2])\n",
    "    scores['test_f1_score_neg'].append(f1[0])\n",
    "    scores['test_f1_score_neut'].append(f1[1])\n",
    "    scores['test_f1_score_pos'].append(f1[2])\n",
    "    #debug: print score every round\n",
    "    #print(scores) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate DataFrame of the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision_neg</th>\n",
       "      <th>test_precision_neut</th>\n",
       "      <th>test_precision_pos</th>\n",
       "      <th>test_recall_neg</th>\n",
       "      <th>test_recall_neut</th>\n",
       "      <th>test_recall_pos</th>\n",
       "      <th>test_f1_score_neg</th>\n",
       "      <th>test_f1_score_neut</th>\n",
       "      <th>test_f1_score_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.796686</td>\n",
       "      <td>0.809655</td>\n",
       "      <td>0.769824</td>\n",
       "      <td>0.814507</td>\n",
       "      <td>0.806319</td>\n",
       "      <td>0.722107</td>\n",
       "      <td>0.880463</td>\n",
       "      <td>0.807983</td>\n",
       "      <td>0.745203</td>\n",
       "      <td>0.846201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.806791</td>\n",
       "      <td>0.810294</td>\n",
       "      <td>0.807814</td>\n",
       "      <td>0.802834</td>\n",
       "      <td>0.804380</td>\n",
       "      <td>0.746341</td>\n",
       "      <td>0.890052</td>\n",
       "      <td>0.807326</td>\n",
       "      <td>0.775862</td>\n",
       "      <td>0.844196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.807117</td>\n",
       "      <td>0.779817</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.835859</td>\n",
       "      <td>0.829847</td>\n",
       "      <td>0.738739</td>\n",
       "      <td>0.874505</td>\n",
       "      <td>0.804054</td>\n",
       "      <td>0.769953</td>\n",
       "      <td>0.854745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.782450</td>\n",
       "      <td>0.751538</td>\n",
       "      <td>0.810323</td>\n",
       "      <td>0.786441</td>\n",
       "      <td>0.855742</td>\n",
       "      <td>0.648091</td>\n",
       "      <td>0.881013</td>\n",
       "      <td>0.800262</td>\n",
       "      <td>0.720183</td>\n",
       "      <td>0.831045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.792155</td>\n",
       "      <td>0.756188</td>\n",
       "      <td>0.795535</td>\n",
       "      <td>0.824324</td>\n",
       "      <td>0.862994</td>\n",
       "      <td>0.683838</td>\n",
       "      <td>0.865806</td>\n",
       "      <td>0.806069</td>\n",
       "      <td>0.735470</td>\n",
       "      <td>0.844556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.785685</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.800921</td>\n",
       "      <td>0.764505</td>\n",
       "      <td>0.820257</td>\n",
       "      <td>0.684366</td>\n",
       "      <td>0.890066</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.738070</td>\n",
       "      <td>0.822521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.818035</td>\n",
       "      <td>0.792582</td>\n",
       "      <td>0.814776</td>\n",
       "      <td>0.845663</td>\n",
       "      <td>0.854815</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.858808</td>\n",
       "      <td>0.822523</td>\n",
       "      <td>0.788123</td>\n",
       "      <td>0.852185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.778811</td>\n",
       "      <td>0.747112</td>\n",
       "      <td>0.762876</td>\n",
       "      <td>0.830709</td>\n",
       "      <td>0.842258</td>\n",
       "      <td>0.708873</td>\n",
       "      <td>0.812580</td>\n",
       "      <td>0.791837</td>\n",
       "      <td>0.734884</td>\n",
       "      <td>0.821544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.802669</td>\n",
       "      <td>0.762991</td>\n",
       "      <td>0.837099</td>\n",
       "      <td>0.805457</td>\n",
       "      <td>0.855114</td>\n",
       "      <td>0.699106</td>\n",
       "      <td>0.891076</td>\n",
       "      <td>0.806430</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.846106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.801860</td>\n",
       "      <td>0.792537</td>\n",
       "      <td>0.753991</td>\n",
       "      <td>0.879404</td>\n",
       "      <td>0.791356</td>\n",
       "      <td>0.796627</td>\n",
       "      <td>0.817380</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>0.774723</td>\n",
       "      <td>0.847258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_accuracy  test_precision_neg  test_precision_neut  test_precision_pos  \\\n",
       "0       0.796686            0.809655             0.769824            0.814507   \n",
       "1       0.806791            0.810294             0.807814            0.802834   \n",
       "2       0.807117            0.779817             0.803922            0.835859   \n",
       "3       0.782450            0.751538             0.810323            0.786441   \n",
       "4       0.792155            0.756188             0.795535            0.824324   \n",
       "5       0.785685            0.793103             0.800921            0.764505   \n",
       "6       0.818035            0.792582             0.814776            0.845663   \n",
       "7       0.778811            0.747112             0.762876            0.830709   \n",
       "8       0.802669            0.762991             0.837099            0.805457   \n",
       "9       0.801860            0.792537             0.753991            0.879404   \n",
       "\n",
       "   test_recall_neg  test_recall_neut  test_recall_pos  test_f1_score_neg  \\\n",
       "0         0.806319          0.722107         0.880463           0.807983   \n",
       "1         0.804380          0.746341         0.890052           0.807326   \n",
       "2         0.829847          0.738739         0.874505           0.804054   \n",
       "3         0.855742          0.648091         0.881013           0.800262   \n",
       "4         0.862994          0.683838         0.865806           0.806069   \n",
       "5         0.820257          0.684366         0.890066           0.806452   \n",
       "6         0.854815          0.763158         0.858808           0.822523   \n",
       "7         0.842258          0.708873         0.812580           0.791837   \n",
       "8         0.855114          0.699106         0.891076           0.806430   \n",
       "9         0.791356          0.796627         0.817380           0.791946   \n",
       "\n",
       "   test_f1_score_neut  test_f1_score_pos  \n",
       "0            0.745203           0.846201  \n",
       "1            0.775862           0.844196  \n",
       "2            0.769953           0.854745  \n",
       "3            0.720183           0.831045  \n",
       "4            0.735470           0.844556  \n",
       "5            0.738070           0.822521  \n",
       "6            0.788123           0.852185  \n",
       "7            0.734884           0.821544  \n",
       "8            0.761905           0.846106  \n",
       "9            0.774723           0.847258  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foldOutput = pd.DataFrame(scores)\n",
    "foldOutput.to_csv('foldOutput.csv')\n",
    "foldOutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate averages of the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acthegreat/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3472: FutureWarning: In a future version, DataFrame.mean(axis=None) will return a scalar mean over the entire DataFrame. To retain the old behavior, use 'frame.mean(axis=0)' or just 'frame.mean()'\n",
      "  return mean(axis=axis, dtype=dtype, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "test_accuracy          0.797226\n",
       "test_precision_neg     0.779582\n",
       "test_precision_neut    0.795708\n",
       "test_precision_pos     0.818970\n",
       "test_recall_neg        0.832308\n",
       "test_recall_neut       0.719125\n",
       "test_recall_pos        0.866175\n",
       "test_f1_score_neg      0.804488\n",
       "test_f1_score_neut     0.754437\n",
       "test_f1_score_pos      0.841036\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.mean(foldOutput)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
